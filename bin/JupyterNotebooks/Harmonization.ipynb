{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b9f38-3220-420a-9de9-2824d2c95fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot extract --method MIREOT --input cpc21.owl --lower-term GO:CPC21Code49 --output cpc-transport.owl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02aa8d4-365e-465a-940d-896657cb8c3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filter ICF maps into groups for experiments as bridging axioms\n",
    "\n",
    "As a programmer, write the following python code without executing it. \n",
    "\n",
    "\n",
    "topdir = '/Users/tu/workspace/harmonization'   \n",
    "mapdir = F\"{topdir}/maps/anatomy\"   \n",
    "ontdir = F\"{topdir}/ontology/renamedIRIs\"   \n",
    "\n",
    "Define isLeafNode(iri) as as Boolean function that's true iff len(iri.descendants()) == 1  \n",
    "\n",
    "using the owlready2 API, read an OWL file F\"{ontdir}/whofic-renamed.owl\" as ont,   \n",
    "Read the mapping table F\"{mapdir}/ICF-ICD-anatomy-v1.xlsx\" Excel spreadsheet.   \n",
    "Use the first row as the header row.   \n",
    "1. Define 6 new dataframes DF1, DF2, DF3, DF4, DF5, DF6   \n",
    "2. Define SurfaceTopIRI as onto.search_one(iri='http://id.who.int/icd/entity/687250607')   \n",
    "3. For each row after the header row,   \n",
    "      subjectIRI = onto.search_one(iri='http://id.who.int/icf/entity/' + (subject_id as a string)) of the row   \n",
    "      objectIRI = onto.search_one(iri='http://id.who.int/icd/entity/' + (object_id as a string)) of the row   \n",
    "4. If objectIRI is a descendant of SurfaceTopIRI, then     \n",
    "   add the row to DF1 using the panda.concat function    \n",
    "   else add the row to DF2 using the panda.concat function   \n",
    "5. if isLeafNode(subjectIRI) is true   \n",
    "    add the row to DF3 using the panda.concat function   \n",
    "  else add the row to DF4   \n",
    "6. if (the objectIRI is a descendant of SurfaceTopIRI and not isLeafNode(subjectIRI) )   \n",
    "    add the row to DF5   \n",
    "  else add the row to DF6   \n",
    "7. Write each of (DF1, DF2, DF3, DF4, DF5, DF6) as excel files   \n",
    "  (TargetSurfaceTopography.xlsx, TargetNotSurfaceTopography.xlsx, SourceLeafNode.xlsx,   \n",
    "   SourceNotLeafNode.xlsx, TargetSurfaceTopographySourceNotLeafNode.xlsx,     \n",
    "   TargetNotSurfaceTopographyOrSourceLeafNode.xlsx)   \n",
    "respectively without the index column in mapdir  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3680f842-f9a1-48f2-a0b4-b5628551b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from owlready2 import *\n",
    "\n",
    "# Initialize directories\n",
    "topdir = '/Users/tu/workspace/harmonization'\n",
    "mapdir = f\"{topdir}/maps/anatomy\"\n",
    "ontdir = f\"{topdir}/ontology/renamedIRIs\"\n",
    "\n",
    "# Function that checks if IRI is a leaf node\n",
    "def isLeafNode(iri):\n",
    "    return len(iri.descendants()) == 1\n",
    "\n",
    "# Load the ontology\n",
    "onto = get_ontology(f\"{ontdir}/whofic-renamed.owl\").load()\n",
    "\n",
    "# Read the Excel mapping table\n",
    "df = pd.read_excel(f\"{mapdir}/ICF-ICD-anatomy-v1.xlsx\", header=0)\n",
    "\n",
    "# Define new DataFrames\n",
    "DF1 = pd.DataFrame()\n",
    "DF2 = pd.DataFrame()\n",
    "DF3 = pd.DataFrame()\n",
    "DF4 = pd.DataFrame()\n",
    "DF5 = pd.DataFrame()\n",
    "DF6 = pd.DataFrame()\n",
    "\n",
    "# Define SurfaceTopIRI\n",
    "SurfaceTopIRI = onto.search_one(iri='http://id.who.int/icd/entity/687250607')\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    subjectIRI = onto.search_one(iri='http://id.who.int/icf/entity/' + str(row['subject_id']))\n",
    "    objectIRI = onto.search_one(iri='http://id.who.int/icd/entity/' + str(row['object_id']))\n",
    "\n",
    "    # Check conditions and add row to corresponding dataframe\n",
    "    if SurfaceTopIRI in objectIRI.ancestors(): # If objectIRI is a descendant of SurfaceTopIRI\n",
    "        DF1 = pd.concat([DF1, df.loc[_:_]])\n",
    "    else:\n",
    "        DF2 = pd.concat([DF2, df.loc[_:_]])\n",
    "\n",
    "    if isLeafNode(subjectIRI): # If subjectIRI is a leafnode\n",
    "        DF3 = pd.concat([DF3, df.loc[_:_]])\n",
    "    else:\n",
    "        DF4 = pd.concat([DF4, df.loc[_:_]])\n",
    "\n",
    "    if SurfaceTopIRI in objectIRI.ancestors() and not isLeafNode(subjectIRI): # If objectIRI is a descendant of SurfaceTopIRI and subjectIRI is a leafnode\n",
    "        DF5 = pd.concat([DF5, df.loc[_:_]])\n",
    "    else:\n",
    "        DF6 = pd.concat([DF6, df.loc[_:_]])\n",
    "\n",
    "# Write Dataframes to Excel files\n",
    "DF1.to_excel(f\"{mapdir}/TargetSurfaceTopography.xlsx\", index=False)\n",
    "DF2.to_excel(f\"{mapdir}/TargetNotSurfaceTopography.xlsx\", index=False)\n",
    "DF3.to_excel(f\"{mapdir}/SourceLeafNode.xlsx\", index=False)\n",
    "DF4.to_excel(f\"{mapdir}/SourceNotLeafNode.xlsx\", index=False)\n",
    "DF5.to_excel(f\"{mapdir}/TargetSurfaceTopographySourceNotLeafNode.xlsx\", index=False)\n",
    "DF6.to_excel(f\"{mapdir}/TargetNotSurfaceTopographyOrSourceLeafNode.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c552c2-906e-4890-a4e3-d9e9c2c21337",
   "metadata": {},
   "source": [
    "## Filter ICHI maps into groups for experiments as bridging axioms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d67e684-1a69-4ecd-ab50-b916262d24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from owlready2 import *\n",
    "\n",
    "# Initialize directories\n",
    "topdir = '/Users/tu/workspace/harmonization'\n",
    "mapdir = f\"{topdir}/maps/anatomy\"\n",
    "ontdir = f\"{topdir}/ontology/renamedIRIs\"\n",
    "classification = \"ICHI\"\n",
    "\n",
    "# Function that checks if IRI is a leaf node\n",
    "def isLeafNode(iri):\n",
    "    return len(iri.descendants()) == 1\n",
    "\n",
    "# Load the ontology\n",
    "onto = get_ontology(f\"{ontdir}/whofic-renamed.owl\").load()\n",
    "\n",
    "# Read the Excel mapping table\n",
    "df = pd.read_excel(f\"{mapdir}/ICHI-ICD-anatomy-v3.xlsx\", header=0)\n",
    "\n",
    "# Define new DataFrames\n",
    "DF1 = pd.DataFrame()\n",
    "DF2 = pd.DataFrame()\n",
    "DF3 = pd.DataFrame()\n",
    "DF4 = pd.DataFrame()\n",
    "DF5 = pd.DataFrame()\n",
    "DF6 = pd.DataFrame()\n",
    "\n",
    "# Define SurfaceTopIRI\n",
    "SurfaceTopIRI = onto.search_one(iri='http://id.who.int/icd/entity/687250607')\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    subjectIRI = onto.search_one(iri='http://id.who.int/ichi/entity/' + str(row['source']))\n",
    "    objectIRI = onto.search_one(iri='http://id.who.int/icd/entity/' + str(row['target']))\n",
    "\n",
    "    # Check conditions and add row to corresponding dataframe\n",
    "    if SurfaceTopIRI in objectIRI.ancestors(): # If objectIRI is a descendant of SurfaceTopIRI\n",
    "        DF1 = pd.concat([DF1, df.loc[_:_]])\n",
    "    else:\n",
    "        DF2 = pd.concat([DF2, df.loc[_:_]])\n",
    "\n",
    "    if isLeafNode(subjectIRI): # If subjectIRI is a leafnode\n",
    "        DF3 = pd.concat([DF3, df.loc[_:_]])\n",
    "    else:\n",
    "        DF4 = pd.concat([DF4, df.loc[_:_]])\n",
    "\n",
    "    if SurfaceTopIRI in objectIRI.ancestors() and not isLeafNode(subjectIRI): # If objectIRI is a descendant of SurfaceTopIRI and subjectIRI is a leafnode\n",
    "        DF5 = pd.concat([DF5, df.loc[_:_]])\n",
    "    else:\n",
    "        DF6 = pd.concat([DF6, df.loc[_:_]])\n",
    "\n",
    "# Write Dataframes to Excel files\n",
    "DF1.to_excel(f\"{mapdir}/ICHITargetSurfaceTopography.xlsx\", index=False)\n",
    "DF2.to_excel(f\"{mapdir}/ICHITargetNotSurfaceTopography.xlsx\", index=False)\n",
    "DF3.to_excel(f\"{mapdir}/ICHISourceLeafNode.xlsx\", index=False)\n",
    "DF4.to_excel(f\"{mapdir}/ICHISourceNotLeafNode.xlsx\", index=False)\n",
    "DF5.to_excel(f\"{mapdir}/ICHITargetSurfaceTopographySourceNotLeafNode.xlsx\", index=False)\n",
    "DF6.to_excel(f\"{mapdir}/ICHITargetNotSurfaceTopographyOrSourceLeafNode.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7705b-767d-4fd5-8774-d1dd8fe67f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf040998-958d-4bd3-bf1b-858b6843fe1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add ICF-body-structure-to-ICD-anatomy maps as bridging axioms (if ICF sources are leaf nodes)\n",
    "As a programmer, write the following python code without executing it. using the owlready2 API, read an OWL file from ontology/whofic-anatomy-harmonization.owl, and the mapping table map/anatomy/ICF-ICD-anatomy-v1 Excel spreadsheet. Use the first row as the header row. For each row after the header row, use the owlread2 API add OWL axiom to the loaded ontology as follows:\n",
    "1. subjectIRI = 'http://id.who.int/icf/entity/' + (subject_id as a string) of the row\n",
    "2. objectIRI = 'http://id.who.int/icd/entity/' + (object_id as a string) of the row\n",
    "3. If subjectIRI is a leaf node, then\n",
    "3.1 . If predicate_id is 'skos:BroadMatch' then add the axiom objectIRI isSubClassOf subjectIRI\n",
    "3.2. If predicate_id is 'skos:NarrowMatch' then add the axiom subjectIRI isSubClassOf objectIRI\n",
    "3.3 . If the predicate_id is 'skos:ExactMatch', then add the axiom subjectIRI isEquivalentTo objectIRI\n",
    "endif\n",
    "Save the ontology to the file whofic-anatomy-harmonization-1.owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a8a80d-91ee-4d8b-8d74-8a4d70d1dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdir = '/Users/tu/workspace/harmonization'\n",
    "mapdir = F\"{topdir}/maps/anatomy\"\n",
    "ontdir = F\"{topdir}/ontology/renamedIRIs\"\n",
    "\n",
    "import pandas as pd\n",
    "from owlready2 import *\n",
    "\n",
    "# Load the ontology\n",
    "ontology = get_ontology(F\"{ontdir}/whofic-renamed.owl\").load()\n",
    "\n",
    "# Load the Excel mapping file\n",
    "df = pd.read_excel(F\"{mapdir}/ICF-ICD-anatomy-v1.xlsx\")\n",
    "\n",
    "# Iterate rows in DataFrame\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    subject_iri = IRIS['http://id.who.int/icf/entity/' + str(row['subject_id'])]\n",
    "    object_iri = IRIS['http://id.who.int/icd/entity/' + str(row['object_id'])]\n",
    "\n",
    "    if not subject_iri.descendants():\n",
    "        print(F\"{subject_iri} {row['predicate_id']} {object_iri}\")\n",
    "        count = count + 1\n",
    "    # Create corresponding axioms based on predicate_id\n",
    "        if row['predicate_id'] == 'skos:BroadMatch':\n",
    "            axiom = object_iri.is_a.append(subject_iri) \n",
    "        elif row['predicate_id'] == 'skos:NarrowMatch':\n",
    "            axiom = subject_iri.is_a.append(object_iri) \n",
    "        elif row['predicate_id'] == 'skos:ExactMatch':\n",
    "            axiom = subject_iri.equivalent_to.append(object_iri)\n",
    "print(\"Number of axioms: \"+ count)\n",
    "# Save the ontology with the new axioms\n",
    "ontology.save(F\"{ontdir}/whofic-anatomy-icfleaves-harmonization.owl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3950350e-c459-47fe-8060-5053c0d13322",
   "metadata": {},
   "source": [
    "## Add ICF-body-structure-to-ICD-anatomy maps as bridging axioms (if ICD targets are not part of Surface topography)\n",
    "As a programmer, write the following python code without executing it. using the owlready2 API, read an OWL file from ontology/whofic-anatomy-harmonization.owl, and the mapping table map/anatomy/ICF-ICD-anatomy-v1 Excel spreadsheet. Use the first row as the header row. For each row after the header row, use the owlread2 API add OWL axiom to the loaded ontology as follows:\n",
    "1. subjectIRI = 'http://id.who.int/icf/entity/' + (subject_id as a string) of the row\n",
    "2. objectIRI = 'http://id.who.int/icd/entity/' + (object_id as a string) of the row\n",
    "3. If objectIRI is not a descendant of http://id.who.int/icd/entity/687250607, then\n",
    "3.1 . If predicate_id is 'skos:BroadMatch' then add the axiom objectIRI isSubClassOf subjectIRI\n",
    "3.2. If predicate_id is 'skos:NarrowMatch' then add the axiom subjectIRI isSubClassOf objectIRI\n",
    "3.3 . If the predicate_id is 'skos:ExactMatch', then add the axiom subjectIRI isEquivalentTo objectIRI\n",
    "endif\n",
    "Save the ontology to the file whofic-anatomy-nosurfacet-harmonization.owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4659a21-122d-43fa-aa1b-c292bb867171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from owlready2 import *\n",
    "\n",
    "topdir = '/Users/tu/workspace/harmonization'\n",
    "mapdir = F\"{topdir}/maps/anatomy\"\n",
    "ontdir = F\"{topdir}/ontology/renamedIRIs\"\n",
    "\n",
    "# Load the ontology\n",
    "ontology = get_ontology(F\"{ontdir}/whofic-renamed.owl\").load()\n",
    "\n",
    "# Load the Excel mapping file\n",
    "df = pd.read_excel(F\"{mapdir}/ICF-ICD-anatomy-v1.xlsx\")\n",
    "\n",
    "# Reference IRI for comparison\n",
    "compareIRI = onto['http://id.who.int/icd/entity/687250607'] \n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    subject_iri = IRIS['http://id.who.int/icf/entity/' + str(row['subject_id'])]\n",
    "    object_iri = IRIS['http://id.who.int/icd/entity/' + str(row['object_id'])]\n",
    "    predicate_id = str(row['predicate_id'])\n",
    "\n",
    "    # Construct the IRI\n",
    "    subjectIRI = onto['http://id.who.int/icf/entity/' + subject_id]\n",
    "    objectIRI = onto['http://id.who.int/icd/entity/' + object_id]\n",
    "\n",
    "    # Check if objectIRI is not a descendant of compareIRI:\n",
    "    if compareIRI not in objectIRI.ancestors(): \n",
    "\n",
    "        # Add axioms based on predicate_id \n",
    "        if predicate_id == 'skos:BroadMatch':\n",
    "            objectIRI.is_a.append(subjectIRI)\n",
    "        elif predicate_id == 'skos:NarrowMatch':\n",
    "            subjectIRI.is_a.append(objectIRI)\n",
    "        elif predicate_id == 'skos:ExactMatch':\n",
    "            EquivalentTo([subjectIRI, objectIRI])\n",
    "\n",
    "# Save the updated ontology\n",
    "onto.save(\"whofic-anatomy-nosurfacet-harmonization.owl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c881e4-9c85-4bc8-afd7-b755a8c0c398",
   "metadata": {},
   "source": [
    "## Add ICF-body-structure-to-ICD-anatomy maps as bridging axioms (if ICD targets are not part of Surface topography or if the ICF source is a leaf node)\n",
    "As a programmer, write the following python code without executing it. \n",
    "topdir = '/Users/tu/workspace/harmonization' \n",
    "mapdir = F\"{topdir}/maps/anatomy\" \n",
    "ontdir = F\"{topdir}/ontology/renamedIRIs\" \n",
    "\n",
    "using the owlready2 API, read an OWL file from F\"{ontdir}whofic-renamed.owl\", \n",
    "Read the mapping table F\"{mapdir}TargetNotSurfaceTopographyOrSourceLeafNode\" Excel spreadsheet. \n",
    "\n",
    "Use the first row as the header row. \n",
    "  subjectIRI = onto.search_one(iri='http://id.who.int/icf/entity/' + (subject_id as a string)) of the row \n",
    "  objectIRI = onto.search_one(iri='http://id.who.int/icd/entity/' + (object_id as a string)) of the row \n",
    "  \n",
    "  For each row after the header row, use the owlread2 API to add OWL axiom to the loaded ontology as follows:\n",
    "  If predicate_id is 'skos:BroadMatch' then add the axiom objectIRI isSubClassOf subjectIRI\n",
    "  If predicate_id is 'skos:NarrowMatch' then add the axiom subjectIRI isSubClassOf objectIRI\n",
    "  If the predicate_id is 'skos:ExactMatch', then add the axiom subjectIRI isEquivalentTo objectIRI\n",
    "\n",
    "Save the ontology to the file whofic-anatomy-nosurftop-or-leaf-harmonization.owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc920214-d69d-4961-b7a4-062c8eea1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from owlready2 import *\n",
    "\n",
    "# Define directories\n",
    "topdir = '/Users/tu/workspace/harmonization' \n",
    "mapdir = f\"{topdir}/maps/anatomy\" \n",
    "ontdir = f\"{topdir}/ontology/renamedIRIs\" \n",
    "\n",
    "# Load the ontology\n",
    "onto = get_ontology(f\"{ontdir}/whofic-renamed.owl\").load()\n",
    "\n",
    "# Load the Excel mapping table\n",
    "df = pd.read_excel(f\"{mapdir}/TargetNotSurfaceTopographyOrSourceLeafNode.xlsx\", header=0)\n",
    "count = 0\n",
    "# Iterate over each row in the file (skipping the header row)\n",
    "for _, row in df.iterrows():\n",
    "    # Create the IRI for each subject and object\n",
    "    subjectIRI = onto.search_one(iri='http://id.who.int/icf/entity/' + str(row['subject_id']))\n",
    "    objectIRI = onto.search_one(iri='http://id.who.int/icd/entity/' + str(row['object_id']))\n",
    "\n",
    "    # Add the axioms to the ontology based on the predicate_id\n",
    "    predicate_id = row['predicate_id']\n",
    "    if predicate_id == 'skos:BroadMatch':\n",
    "        objectIRI.is_a.append(subjectIRI)\n",
    "    elif predicate_id == 'skos:NarrowMatch':\n",
    "        subjectIRI.is_a.append(objectIRI)\n",
    "    elif predicate_id == 'skos:ExactMatch':\n",
    "        subjectIRI.equivalent_to.append(objectIRI)\n",
    "\n",
    "# Save the modified ontology\n",
    "onto.save(F\"{ontdir}/whofic-anatomy-nosurftop-or-leaf-harmonization.owl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4936af-6535-4449-ae2e-27730a761650",
   "metadata": {},
   "source": [
    "## Add ICHI anatomy-target-to-ICD-anatomy maps as bridging axioms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48467fc-529e-4e09-a515-2e850bf1ba46",
   "metadata": {},
   "source": [
    "Check to see if there is any cycles in the ICHI-ICD mappings\n",
    "Write the python code to do the following:\n",
    "- topdir = '/Users/tu/workspace/harmonization'\n",
    "- mapdir = F\"{topdir}/maps/anatomy\"\n",
    "- read the excel file F\"{mapdir}/ICHI-ICD-anatomy-v1.xlsx\"\n",
    "- Use the first row as the header row. \n",
    "- For the following rows, check if there are two rows where source and target are the same but predicate_id are different\n",
    "- print out such pairs of rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914f0086-0383-493d-b223-2c4624409c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Directories\n",
    "topdir = '/Users/tu/workspace/harmonization'\n",
    "mapdir = f\"{topdir}/maps/anatomy\"\n",
    "\n",
    "# Read the excel file\n",
    "df = pd.read_excel(f\"{mapdir}/ICHI-ICD-anatomy-v1.xlsx\")\n",
    "\n",
    "# Group the DataFrame by 'subject_id' and 'object_id'\n",
    "grouped = df.groupby(['source', 'target'])\n",
    "\n",
    "# Find groups where 'predicate_id' has more than one unique value\n",
    "for name, group in grouped:\n",
    "    if group['predicate_id'].nunique() > 1:\n",
    "        print(group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51117ea9-cb5d-4a6d-bc2a-3a0dc139592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "source http://id.who.int/ichi/entity/1154085341; target http://id.who.int/icd/entity/947150833; predicate na\n",
      "source http://id.who.int/ichi/entity/1154085341; target http://id.who.int/icd/entity/40907641; predicate na\n"
     ]
    }
   ],
   "source": [
    "topdir = '/Users/tu/workspace/harmonization'\n",
    "mapdir = F\"{topdir}/maps/anatomy\"\n",
    "ontdir = F\"{topdir}/ontology/renamedIRIs\"\n",
    "\n",
    "import pandas as pd\n",
    "from owlready2 import *\n",
    "\n",
    "# Load the ontology\n",
    "ontology = get_ontology(F\"{ontdir}/whofic-renamed.owl\").load()\n",
    "\n",
    "# Load the Excel mapping file\n",
    "df = pd.read_excel(F\"{mapdir}/ICHI-ICD-anatomy-v1.xlsx\")\n",
    "\n",
    "# Iterate rows in DataFrame\n",
    "for idx, row in df.iterrows():\n",
    "    subject_iri = IRIS[row['source']]\n",
    "    object_iri = IRIS[row['target']]\n",
    "\n",
    "\n",
    "    # Create corresponding axioms based on predicate_id\n",
    "    try: \n",
    "        if row['predicate_id'] == 'br':\n",
    "            axiom = object_iri.is_a.append(subject_iri) \n",
    "        elif row['predicate_id'] == 'eq':\n",
    "            axiom = subject_iri.equivalent_to.append(object_iri)\n",
    "        elif row['predicate_id'] == 'na':\n",
    "            axiom = subject_iri.is_a.append(object_iri)\n",
    "    except:\n",
    "        print(f\"source {row['source']}; target {row['target']}; predicate {row['predicate_id']}\")\n",
    "\n",
    "# Save the ontology with the new axioms\n",
    "ontology.save(F\"{ontdir}/whofic-ichi-icd-anatomy-harmonization.owl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b5ac16f-50b0-465e-bd13-fa1d933450f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity.1093096992\n",
      "entity.1204996303\n",
      "entity.1213923926\n",
      "entity.2034447649\n",
      "entity.1308284441\n",
      "entity.1358605327\n",
      "entity.1366916688\n",
      "entity.1385726880\n",
      "entity.1474796775\n",
      "entity.1601398422\n",
      "entity.1934266287\n",
      "entity.1656025598\n",
      "entity.1712049110\n",
      "entity.1799284202\n",
      "entity.1855926695\n",
      "entity.1898163990\n",
      "entity.1973196094\n",
      "entity.2042058087\n",
      "entity.207306511\n",
      "entity.2081849642\n",
      "entity.351965526\n",
      "entity.460544975\n",
      "entity.46930929\n",
      "entity.604514306\n",
      "entity.622249575\n",
      "entity.842965671\n",
      "entity.891523663\n"
     ]
    }
   ],
   "source": [
    "subject_iri = IRIS['http://id.who.int/ichi/entity/1652076574']\n",
    "object_iri = IRIS[ 'http://id.who.int/icd/entity/1885434231']\n",
    "for c in object_iri.subclasses():\n",
    "    print(c)\n",
    "subject_iri.isa.append(object_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c326f-5177-449b-a947-51f2c4fa1578",
   "metadata": {},
   "source": [
    "## Change the IRIs of ICF and ICHI entities so that they are distinguuishable\n",
    "Use the owlready2 API, read an OWL file whofic-2024-01-21.owl\n",
    "find all descendants of 'http://id.who.int/icd/entity/1405434703' and change their names from 'http://id.who.int/icd/entity/nnnnnnnnn' to 'http://id.who.int/icf/entity/nnnnnnnnn'. save the ontology as whofic-renamed.owl. Ditto for ICHI entities (except anatomy and topography, which ICHI shares with ICD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7267e5ca-cf3a-4ab5-972f-103d9098e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "\n",
    "dir = \"/Users/tu/workspace/evalicdmappings/ontologies/\"\n",
    "# Load your ontology\n",
    "onto = get_ontology(f\"file://{dir}whofic-2024-01-21.owl\").load()\n",
    "\n",
    "# top-level ICF \n",
    "class_to_change = onto.search_one(iri=\"http://id.who.int/icd/entity/1405434703\")\n",
    "if class_to_change:\n",
    "    # Change the name of the class itself\n",
    "    class_to_change.iri = class_to_change.iri.replace('/icd/', '/ichi/')\n",
    "\n",
    "    # Fetch subclasses and change IRIs\n",
    "    for sub_class in class_to_change.descendants():\n",
    "        old_iri = sub_class.iri\n",
    "        new_iri = old_iri.replace(\"/icd/\", \"/icf/\")\n",
    "        sub_class.iri = new_iri\n",
    "\n",
    "# top-level ICHI minus anatomy & topography and topology\n",
    "classes_to_rename = [\n",
    "    \"http://id.who.int/icd/entity/1190112446\",\n",
    "    \"http://id.who.int/icd/entity/1399457851\",\n",
    "    \"http://id.who.int/icd/entity/1338961517\",\n",
    "    \"http://id.who.int/icd/entity/337502477\",\n",
    "    \"http://id.who.int/icd/entity/974232433\",\n",
    "    \"http://id.who.int/icd/entity/448900003\",\n",
    "    \"http://id.who.int/icd/entity/89604631\"\n",
    "]\n",
    "\n",
    "for class_iri in classes_to_rename:\n",
    "    class_to_change = onto.search_one(iri=class_iri)\n",
    "    \n",
    "    if class_to_change:\n",
    "        # Change the name of the class itself\n",
    "        class_to_change.iri = class_to_change.iri.replace('/icd/', '/ichi/')\n",
    "\n",
    "        # Change the names of all its descendants (subclasses)\n",
    "        for descendant in class_to_change.descendants():\n",
    "            descendant.iri = descendant.iri.replace('/icd/', '/ichi/')\n",
    "\n",
    "# Save the modified ontology\n",
    "onto.save(f\"{dir}whofic-renamed.owl\", format=\"rdfxml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d5f99-6506-45dc-8ab2-b67dfbf06a3f",
   "metadata": {},
   "source": [
    "## Add rdfs:label to ICD transport tree, using skos:prefLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcbbe23-c78a-4819-bf7a-1a6145a03896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "\n",
    "#inputFile = \"/Users/tu/workspace/harmonization/ontology/CPC/icd-transport-equipment.owl\"\n",
    "inputFile = \"/Users/tu/Documents/Dropbox/Ontologies/ICD11.owl\"\n",
    "#inputFile = \"/Users/tu/Documents/Dropbox/Ontologies/ICD10.owl\"\n",
    "outputFile = \"/Users/tu/Documents/Dropbox/Ontologies/ICD11-label.owl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c220f36-aba1-4b33-b016-4176e12fba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto=get_ontology(inputFile).load()\n",
    "classes = onto.classes()\n",
    "for c in classes:\n",
    "    c.label = c.prefLabel\n",
    "onto.save(file = outputFile, format=\"rdfxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ce20e-b8de-4705-bed7-f5fbe3a658d5",
   "metadata": {},
   "source": [
    "## Extracting anatomical/functional targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e25f2dd-b73f-4513-bfde-dbf7ae98b4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tu/Documents/Dropbox/WHO (1)/ICHI/Versions\n",
      "zsh:1: command not found: robot\n"
     ]
    }
   ],
   "source": [
    "%cd '/Users/tu/Documents/Dropbox/WHO (1)/ICHI/Versions/ICHI - IRI'  \n",
    "!robot extract --prefix 'ichi: http://id.who.int/ichi/entity/' --method MIREOT --input ICHI-IRI-Modified.owl --branch-from-term ichi:229313065 --output ICHI-Anatomy-Targets.owl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e2e3a4d-fa3b-422d-bb46-f6cbfacfd804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADDED ENTRIES (in 2023 but not in 2018):\n",
      "Code\tText\n",
      "00.66\tPercutaneous transluminal coronary angioplasty [PTCA] or coronary\n",
      "00.70\tRevision of hip replacement, both acetabular and femoral components\n",
      "00.71\tRevision of hip replacement, acetabular component\n",
      "00.72\tRevision of hip replacement, femoral component\n",
      "00.73\tRevision of hip replacement, acetabular liner and/or femoral head only\n",
      "00.74\tHip replacement bearing surface, metal on polyethylene\n",
      "00.75\tHip replacement bearing surface, metal-on-metal\n",
      "00.76\tHip replacement bearing surface, ceramic-on-ceramic\n",
      "00.77\tHip replacement bearing surface, ceramic-on-polyethylene\n",
      "13.1\tIntracapsular extraction of lens\n",
      "13.2\tExtracapsular extraction of lens by linear extraction technique\n",
      "13.3\tExtracapsular extraction of lens by simple aspiration (and irrigation) technique\n",
      "13.4\tExtracapsular extraction of lens by fragmentation and aspiration technique\n",
      "13.5\tOther extracapsular extraction of lens\n",
      "13.6\tOther cataract extraction\n",
      "13.7\tInsertion of prosthetic lens [pseudophakos]\n",
      "13.8\tRemoval of implanted lens\n",
      "17.1\tLaparoscopic unilateral repair of inguinal hernia\n",
      "17.2\tLaparoscopic bilateral repair of inguinal hernia\n",
      "28.2\tTonsillectomy without adenoidectomy\n",
      "28.3\tTonsillectomy with adenoidectomy\n",
      "28.4\tExcision of tonsil tag\n",
      "36.01\tSingle vessel percutaneous transluminal coronary angioplasty [PTCA] or\n",
      "36.02\tSingle vessel percutaneous transluminal coronary angioplasty [PTCA] or\n",
      "36.05\tMultiple vessel percutaneous transluminal coronary angioplasty [PTCA] or\n",
      "36.1\tBypass anastomosis for heart revascularization\n",
      "41.0\tBone marrow or hematopoietic stem cell transplant\n",
      "47.0\tAppendectomy\n",
      "47.01\tLaparoscopic appendectomy\n",
      "47.1\tIncidental appendectomy\n",
      "47.11\tLaparoscopic incidental appendectomy\n",
      "51.22\tCholecystectomy\n",
      "51.23\tLaparoscopic cholecystectomy\n",
      "53.0\tUnilateral repair of inguinal hernia\n",
      "53.1\tBilateral repair of inguinal hernia\n",
      "60.2\tTransurethral prostatectomy\n",
      "60.3\tSuprapubic prostatectomy\n",
      "60.4\tRetropubic prostatectomy\n",
      "60.5\tRadical prostatectomy\n",
      "60.6\tOther prostatectomy\n",
      "68.3\tSubtotal abdominal hysterectomy\n",
      "68.31\tLaparoscopic supracervical hysterectomy [LSH]\n",
      "68.4\tTotal abdominal hysterectomy\n",
      "68.41\tLaparoscopic total abdominal hysterectomy\n",
      "68.5\tVaginal hysterectomy\n",
      "68.51\tLaparoscopically assisted vaginal hysterectomy (LAVH)\n",
      "68.6\tRadical abdominal hysterectomy\n",
      "68.61\tLaparoscopic radical abdominal hysterectomy\n",
      "68.7\tRadical vaginal hysterectomy\n",
      "68.71\tLaparoscopic radical vaginal hysterectomy\n",
      "68.9\tOther and unspecified hysterectomy\n",
      "74.0\tClassical cesarean section\n",
      "74.1\tLow cervical cesarean section\n",
      "74.2\tExtraperitoneal cesarean section\n",
      "74.4\tCesarean section of other specified type\n",
      "74.99\tOther cesarean section of unspecified type\n",
      "81.51\tTotal hip replacement\n",
      "81.52\tPartial hip replacement\n",
      "81.53\tRevision of hip replacement\n",
      "81.53\tRevision of hip replacement, not otherwise specified\n",
      "81.54\tTotal knee replacement\n",
      "85.20\tExcision or destruction of breast tissue, not otherwise specified\n",
      "85.21\tLocal excision of lesion of breast\n",
      "85.22\tResection of quadrant of breast\n",
      "85.23\tSubtotal mastectomy\n",
      "85.33\tUnilateral subcutaneous mammectomy with synchronous implant\n",
      "85.34\tOther unilateral subcutaneous mammectomy\n",
      "85.35\tBilateral subcutaneous mammectomy with synchronous implant\n",
      "85.36\tOther bilateral subcutaneous mammectomy\n",
      "85.4\tMastectomy\n",
      "\n",
      "DELETED ENTRIES (in 2018 but not in 2023):\n",
      "Code\tText\n",
      "01.31\tIncision of cerebral meninges\n",
      "01.39\tOther incision of brain (incl. drainage of cerebral haematoma)\n",
      "01.4\tOperations on thalamus and globus pallidus\n",
      "01.5\tOther excision or destruction of brain and meninges\n",
      "06.2\tUnilateral thyroid lobectomy\n",
      "06.3\tOther partial thyroidectomy\n",
      "06.4\tComplete thyroidectomy\n",
      "06.5\tSubsternal thyroidectomy\n",
      "06.6\tExcision of lingual thyroid\n",
      "20.96\tImplantation or replacement of cochlear prosthetic device, not otherwise specified\n",
      "20.97\tImplantation or replacement of cochlear prosthetic device, single channel\n",
      "20.98\tImplantation or replacement of cochlear prosthetic device, multiple channel\n",
      "32.3\tSegmental resection of lung\n",
      "32.4\tLobectomy of lung\n",
      "32.5\tComplete pneumonectomy\n",
      "33.21\tBronchoscopy through artificial stoma\n",
      "33.22\tFiber-optic bronchoscopy\n",
      "33.23\tOther bronchoscopy\n",
      "33.24\tClosed [endoscopic] biopsy of bronchus\n",
      "33.27\tClosed endoscopic biopsy of lung\n",
      "38.12\tCarotid endarterectomy\n",
      "38.34\tResection of vessel with anastomosis (aorta)\n",
      "38.44\tResection of vessel with replacement (aorta)\n",
      "39.29\tOther (peripheral) vascular shunt or bypass (incl femoropopliteal bypass)\n",
      "39.71\tEndovascular implantation of graft in abdominal aorta\n",
      "39.74\tEndovascular repair of aneurysm (aorta, abdominal)\n",
      "45.22\tEndoscopy of large intestine through artificial stoma\n",
      "45.23\tColonoscopy\n",
      "45.24\tFlexible sigmoidoscopy\n",
      "45.25\tClosed [endoscopic] biopsy of large intestine\n",
      "45.42\tEndoscopic polypectomy of large intestine\n",
      "45.43\tEndoscopic destruction of other lesion or tissue of large intestine\n",
      "45.7\tPartial excision of large intestine\n",
      "45.8\tTotal intra-abdominal colectomy\n",
      "80.50\tExcision or destruction of intervertebral disc, unspecified\n",
      "80.51\tExcision of intervetrebral disc\n",
      "80.59\tOther destruction of intervetrebral disc\n",
      "\n",
      "Total added entries: 70\n",
      "Total deleted entries: 37\n",
      "\n",
      "Results saved to:\n",
      "- added_entries.tsv and added_entries.txt\n",
      "- deleted_entries.tsv and deleted_entries.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the two files\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        # First try to read as tab-separated file\n",
    "        df = pd.read_csv(file_path, sep='\\t', header=None, dtype=str, encoding='utf-8')\n",
    "        return df\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # Try with latin-1 encoding\n",
    "            df = pd.read_csv(file_path, sep='\\t', header=None, dtype=str, encoding='latin1')\n",
    "            return df\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # If tab-separated reading fails, try reading as plain text\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        return pd.DataFrame(lines, columns=['entry'])\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        return pd.DataFrame(lines, columns=['entry'])\n",
    "\n",
    "# Function to extract code and text from an entry\n",
    "def extract_code_and_text(entry):\n",
    "    # Attempt to split the entry by whitespace\n",
    "    # Assumes the code comes first, followed by text\n",
    "    parts = entry.strip().split(maxsplit=1)\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0], parts[1]\n",
    "    else:\n",
    "        return entry.strip(), \"\"  # Return just the code if no text is found\n",
    "\n",
    "# File paths\n",
    "f1_path = \"/Users/tu/Documents/Dropbox/WHO (1)/HIRG/JQNMHCShortList2023.txt\"\n",
    "f2_path = \"/Users/tu/Documents/Dropbox/WHO (1)/HIRG/JQNMHCShortList2018.txt\"\n",
    "\n",
    "# Read files\n",
    "df1 = read_file(f1_path)  # 2023 file\n",
    "df2 = read_file(f2_path)  # 2018 file\n",
    "\n",
    "# Create dictionaries to store entries with their code and text\n",
    "entries_2023 = {}\n",
    "entries_2018 = {}\n",
    "\n",
    "# Process 2023 entries\n",
    "if 'entry' in df1.columns:\n",
    "    # If we read it as plain text with 'entry' column\n",
    "    for entry in df1['entry']:\n",
    "        code, text = extract_code_and_text(entry)\n",
    "        entries_2023[entry.strip()] = (code, text)\n",
    "else:\n",
    "    # If we read it as tab-separated\n",
    "    for idx, row in df1.iterrows():\n",
    "        if len(row) >= 2:  # If there are at least 2 columns\n",
    "            code = row[0]\n",
    "            text = row[1]\n",
    "        else:\n",
    "            # If there's only one column, try to split it\n",
    "            entry = row[0]\n",
    "            code, text = extract_code_and_text(entry)\n",
    "        entries_2023[f\"{code} {text}\".strip()] = (code, text)\n",
    "\n",
    "# Process 2018 entries\n",
    "if 'entry' in df2.columns:\n",
    "    # If we read it as plain text with 'entry' column\n",
    "    for entry in df2['entry']:\n",
    "        code, text = extract_code_and_text(entry)\n",
    "        entries_2018[entry.strip()] = (code, text)\n",
    "else:\n",
    "    # If we read it as tab-separated\n",
    "    for idx, row in df2.iterrows():\n",
    "        if len(row) >= 2:  # If there are at least 2 columns\n",
    "            code = row[0]\n",
    "            text = row[1]\n",
    "        else:\n",
    "            # If there's only one column, try to split it\n",
    "            entry = row[0]\n",
    "            code, text = extract_code_and_text(entry)\n",
    "        entries_2018[f\"{code} {text}\".strip()] = (code, text)\n",
    "\n",
    "# Find differences\n",
    "added_entries = set(entries_2023.keys()) - set(entries_2018.keys())\n",
    "deleted_entries = set(entries_2018.keys()) - set(entries_2023.keys())\n",
    "\n",
    "# Create lists for added and deleted entries\n",
    "added_items = [\n",
    "    {\"Code\": entries_2023[entry][0], \"Text\": entries_2023[entry][1]} \n",
    "    for entry in sorted(added_entries)\n",
    "]\n",
    "\n",
    "deleted_items = [\n",
    "    {\"Code\": entries_2018[entry][0], \"Text\": entries_2018[entry][1]} \n",
    "    for entry in sorted(deleted_entries)\n",
    "]\n",
    "\n",
    "# Create DataFrames for added and deleted entries\n",
    "added_df = pd.DataFrame(added_items)\n",
    "deleted_df = pd.DataFrame(deleted_items)\n",
    "\n",
    "# Save as tab-separated files\n",
    "if not added_df.empty:\n",
    "    added_df.to_csv('added_entries.tsv', sep='\\t', index=False, header=True)\n",
    "else:\n",
    "    with open('added_entries.tsv', 'w') as f:\n",
    "        f.write(\"Code\\tText\\n\")\n",
    "\n",
    "if not deleted_df.empty:\n",
    "    deleted_df.to_csv('deleted_entries.tsv', sep='\\t', index=False, header=True)\n",
    "else:\n",
    "    with open('deleted_entries.tsv', 'w') as f:\n",
    "        f.write(\"Code\\tText\\n\")\n",
    "\n",
    "# Print results as tab-separated text\n",
    "print(\"ADDED ENTRIES (in 2023 but not in 2018):\")\n",
    "print(\"Code\\tText\")\n",
    "if added_items:\n",
    "    for item in added_items:\n",
    "        print(f\"{item['Code']}\\t{item['Text']}\")\n",
    "else:\n",
    "    print(\"No added entries found.\")\n",
    "\n",
    "print(\"\\nDELETED ENTRIES (in 2018 but not in 2023):\")\n",
    "print(\"Code\\tText\")\n",
    "if deleted_items:\n",
    "    for item in deleted_items:\n",
    "        print(f\"{item['Code']}\\t{item['Text']}\")\n",
    "else:\n",
    "    print(\"No deleted entries found.\")\n",
    "\n",
    "# Also save as tab-separated text files\n",
    "with open('added_entries.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"ADDED ENTRIES (in 2023 but not in 2018):\\n\")\n",
    "    f.write(\"Code\\tText\\n\")\n",
    "    if added_items:\n",
    "        for item in added_items:\n",
    "            f.write(f\"{item['Code']}\\t{item['Text']}\\n\")\n",
    "    else:\n",
    "        f.write(\"No added entries found.\")\n",
    "\n",
    "with open('deleted_entries.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"DELETED ENTRIES (in 2018 but not in 2023):\\n\")\n",
    "    f.write(\"Code\\tText\\n\")\n",
    "    if deleted_items:\n",
    "        for item in deleted_items:\n",
    "            f.write(f\"{item['Code']}\\t{item['Text']}\\n\")\n",
    "    else:\n",
    "        f.write(\"No deleted entries found.\")\n",
    "\n",
    "print(f\"\\nTotal added entries: {len(added_entries)}\")\n",
    "print(f\"Total deleted entries: {len(deleted_entries)}\")\n",
    "print(\"\\nResults saved to:\")\n",
    "print(\"- added_entries.tsv and added_entries.txt\")\n",
    "print(\"- deleted_entries.tsv and deleted_entries.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349faf67-c715-42fa-9318-ec5a74c340af",
   "metadata": {},
   "source": [
    "Set directory D = '/Users/tu/workspace/evalmappings/bin/CodeFusion_osx-x64_0.9.5/\n",
    "Use panda to read the file:\n",
    "f = f\"{D}/ICHI-NCSP_plus 2024-09-30 maps-checked.xlsx\"\n",
    "Create an output Excel file mismatches.xlsx in directory D\n",
    "for each entry in f add the entry to the output file\n",
    "If the value of column 'ICHI code' is 'NoMatch' and the value of column 'MatchLevel' is not 'NoMatch'\n",
    "or If the value of column 'ICHI code' is not 'NoMatch' and the value of column 'ICHI code' does not match the value in column code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1f90ecd-2ed7-4306-af89-bc6aa278b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatches have been written to /Users/tu/workspace/evalmappings/bin/CodeFusion_osx-x64_0.9.5//mismatches.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Set the directory\n",
    "D = '/Users/tu/workspace/evalmappings/bin/CodeFusion_osx-x64_0.9.5/'\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = f\"{D}/ICHI-NCSP_plus 2024-09-30 maps-checked.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Create a mask for the mismatches based on the conditions provided\n",
    "mismatch_condition = (\n",
    "    (df['ICHI code'] == 'NoMatch') & (df['MatchLevel'] != 'NoMatch') |\n",
    "    (df['ICHI code'] != 'NoMatch') & (df['ICHI code'] != df['code'])\n",
    ")\n",
    "\n",
    "# Filter the DataFrame based on the mismatch condition\n",
    "mismatches = df[mismatch_condition]\n",
    "\n",
    "# Create the output Excel file\n",
    "output_file_path = f\"{D}/mismatches.xlsx\"\n",
    "mismatches.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Mismatches have been written to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
